{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/philko/miniconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = \"HuggingFaceH4/zephyr-7b-beta\" # \"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n",
      "Loading checkpoint shards: 100%|██████████| 8/8 [00:06<00:00,  1.16it/s]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_str, device_map='auto', load_in_8bit=True)\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7408/2287509011.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\nNegative. The review is highly critical of the\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7408/2287509011.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\nNegative. The review is highly critical of the\""
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map:   0%|          | 0/10 [00:00<?, ? examples/s]Asking to pad to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no padding.\n",
      "Asking to truncate to max_length but no maximum length is provided and the model has no predefined maximum length. Default to no truncation.\n",
      "Map: 100%|██████████| 10/10 [00:00<00:00, 1226.80 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Provided list of reviews\n",
    "reviews = [\n",
    "    \"The pizza was horribe and the staff rude. Won't recommend.\",\n",
    "    \"The pasta was undercooked and the service was slow. Not going back.\",\n",
    "    \"The salad was wilted and the waiter was dismissive. Avoid at all costs.\",\n",
    "    \"The soup was cold and the ambiance was noisy. Not a pleasant experience.\",\n",
    "    \"The burger was overcooked and the fries were soggy. I wouldn't suggest this place.\",\n",
    "    \"The sushi was not fresh and the staff seemed uninterested. Definitely not worth it.\",\n",
    "    \"The steak was tough and the wine was sour. A disappointing meal.\",\n",
    "    \"The sandwich was bland and the coffee was lukewarm. Not a fan of this café.\",\n",
    "    \"The dessert was stale and the music was too loud. I won't be returning.\",\n",
    "    \"The chicken was dry and the vegetables were overcooked. A poor dining experience.\"\n",
    "]\n",
    "\n",
    "# Convert the list to a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({'text': reviews})\n",
    "\n",
    "# Load a tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Set tokenizer padding to the left\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Assuming 'test.tokenizer' is your tokenizer object\n",
    "if tokenizer.pad_token is None:\n",
    "    # Add a new pad token if it doesn't exist. This is just an example token.\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7408/2161875571.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=1)[0])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"<s> The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\nNeg\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_7408/1906926966.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=20, output_scores=True, return_dict_in_generate=True)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "output = model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=20, output_scores=True, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"<s> The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\nNegative. The review is highly critical of the pizza and the staff, and the author expresses\"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(e) for e in output.sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 32000])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([tokenizer.encode(text), tokenizer.encode(text)]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 45])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:2 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(inp, max_new_tokens=1, output_scores=True, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.utils.GenerateDecoderOnlyOutput"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5278], [7087]]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_tok = [[tokenizer.encode('positive')[1]], [tokenizer.encode('negative')[1]]]\n",
    "verb_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[5278], [7087]]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 2.6758, 10.0703],\n",
       "        [ 2.6758, 10.0703]], device='cuda:0')"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_res = torch.cat(list(map(lambda i: output.scores[0][:, verb_tok[i]], range(2))), axis=-1)\n",
    "out_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_res = torch.nn.functional.softmax(out_res, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.9999999416759238, 0.9999999416759238]"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(out_res, axis=1, dtype=torch.float64).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1.9999998833518475"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(sum, out_res.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[6.1423e-04, 9.9939e-01],\n",
       "        [6.1423e-04, 9.9939e-01]], device='cuda:0')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0006, 0.0006], device='cuda:0')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_res[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# out_res = torch.nn.functional.softmax(out_res, dim=1)\n",
    "# # TODO: verbalizer for labels\n",
    "# class_probs_combined: Dict[str, torch.Tensor] = {k:torch.sum(out_res[:, v], axis=-1) for k, v in i_dict.items()}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
