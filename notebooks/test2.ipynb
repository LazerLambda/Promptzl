{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_str = \"sshleifer/tiny-gpt2\" # \"openai-community/gpt2\" # \"HuggingFaceH4/zephyr-7b-beta\" # \"openai-community/gpt2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(model_str)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_str, device_map='auto', load_in_8bit=True)\n",
    "# model.to(\"cuda\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11211/2287509011.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n factors factors factors factors factors factors factors factors factors factors\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11211/2287509011.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n factors factors factors factors factors factors factors factors factors factors\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=10)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 10/10 [00:00<00:00, 61.55 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Provided list of reviews\n",
    "reviews = [\n",
    "    \"The pizza was horribe and the staff rude. Won't recommend.\",\n",
    "    \"The pasta was undercooked and the service was slow. Not going back.\",\n",
    "    \"The salad was wilted and the waiter was dismissive. Avoid at all costs.\",\n",
    "    \"The soup was cold and the ambiance was noisy. Not a pleasant experience.\",\n",
    "    \"The burger was overcooked and the fries were soggy. I wouldn't suggest this place.\",\n",
    "    \"The sushi was not fresh and the staff seemed uninterested. Definitely not worth it.\",\n",
    "    \"The steak was tough and the wine was sour. A disappointing meal.\",\n",
    "    \"The sandwich was bland and the coffee was lukewarm. Not a fan of this café.\",\n",
    "    \"The dessert was stale and the music was too loud. I won't be returning.\",\n",
    "    \"The chicken was dry and the vegetables were overcooked. A poor dining experience.\"\n",
    "]\n",
    "\n",
    "# Convert the list to a Hugging Face Dataset\n",
    "dataset = Dataset.from_dict({'text': reviews})\n",
    "\n",
    "# Load a tokenizer\n",
    "# tokenizer = AutoTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Set tokenizer padding to the left\n",
    "tokenizer.padding_side = \"left\"\n",
    "# Assuming 'test.tokenizer' is your tokenizer object\n",
    "if tokenizer.pad_token is None:\n",
    "    # Add a new pad token if it doesn't exist. This is just an example token.\n",
    "    # tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    # model.resize_token_embeddings(len(tokenizer))\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Tokenize the dataset\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(examples[\"text\"], \n",
    "                     padding=\"max_length\", \n",
    "                     truncation=True)\n",
    "\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11211/2161875571.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=1)[0])\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n factors\""
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "tokenizer.decode(model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=1)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_11211/1906926966.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  output = model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=20, output_scores=True, return_dict_in_generate=True)\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "text = \"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n\"\n",
    "output = model.generate(torch.tensor(tokenizer.encode(text, return_tensors=\"pt\")).to('cuda'), max_new_tokens=20, output_scores=True, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[\"The pizza was horribe and the staff rude. Won't recommend.\\n\\nIs this review positive or negative? Elaborate your decision and start the sentence with 'positive' or 'negative'\\n\\n factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors factors\"]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[tokenizer.decode(e) for e in output.sequences]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 50257])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.scores[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "inp = torch.tensor([tokenizer.encode(text), tokenizer.encode(text)]).to('cuda')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50257"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50256"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 41])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inp.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(inp, max_new_tokens=1, output_scores=True, return_dict_in_generate=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "transformers.generation.utils.GenerateDecoderOnlyOutput"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24561], [31591]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_tok = [[tokenizer.encode('positive')[0]], [tokenizer.encode('negative')[0]]]\n",
    "verb_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[24561], [31591]]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "verb_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 0.0113, -0.0019],\n",
       "        [ 0.0113, -0.0019]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_res = torch.cat(list(map(lambda i: output.scores[0][:, verb_tok[i]], range(2))), axis=-1)\n",
    "out_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_res = torch.nn.functional.softmax(out_res, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1.0, 1.0]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(out_res, axis=1, dtype=torch.float64).tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.0"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(map(sum, out_res.cpu().tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5034, 0.4966],\n",
       "        [0.5034, 0.4966]], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5034, 0.5034], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_res[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# out_res = torch.nn.functional.softmax(out_res, dim=1)\n",
    "# # TODO: verbalizer for labtokenizer.encode('positive')els\n",
    "# class_probs_combined: Dict[str, torch.Tensor] = {k:torch.sum(out_res[:, v], axis=-1) for k, v in i_dict.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def infer(model, prompt):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model.generate(prompt, max_new_tokens=1, output_scores=True, return_dict_in_generate=True)\n",
    "    return output.scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate_with_ds(prompt_model, dataloader) -> torch.Tensor:\n",
    "    r\"\"\"Calibrate. See `Paper <https://arxiv.org/abs/2108.02035>`_\n",
    "    \n",
    "    Args:\n",
    "        prompt_model (:obj:`PromptForClassification`): the PromptForClassification model.\n",
    "        dataloader (:obj:`List`): the dataloader to conduct the calibrate, could be a virtual one, i.e. contain an only-template example.\n",
    "    \n",
    "    Return:\n",
    "        (:obj:`torch.Tensor`) A tensor of shape  (vocabsize) or (mask_num, vocabsize), the logits calculated for each word in the vocabulary\n",
    "    \"\"\"\n",
    "    all_logits = []\n",
    "    prompt_model.eval()\n",
    "    for batch in dataloader:\n",
    "        batch = {k:v.to(prompt_model.device) for k, v in batch.items()}\n",
    "        logits = infer(model, batch['input_ids'])\n",
    "        all_logits.append(logits.detach())\n",
    "    all_logits = torch.cat(all_logits, dim=0)\n",
    "    return all_logits.mean(dim=0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenized_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask'])\n",
    "dataloader = DataLoader(tokenized_dataset, batch_size=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text', 'input_ids', 'attention_mask'],\n",
       "    num_rows: 10\n",
       "})"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "mask = torch.ones(len(tokenizer))\n",
    "mask[[element for sublist in verb_tok for element in sublist]] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0.,  ..., 0., 0., 0.])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "1 -  mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def project(\n",
    "        label_words_ids, # get from above\n",
    "        label_words_mask, # get from above\n",
    "        logits,\n",
    "        **kwargs,\n",
    "        ):\n",
    "    label_words_logits = logits[:, label_words_ids]\n",
    "    # label_words_logits = self.handle_multi_token(label_words_logits, label_words_mask)\n",
    "    # print(label_words_logits)\n",
    "    # print(label_words_logits.shape)\n",
    "    # label_words_logits = label_words_logits.select(dim=-1, index=0)\n",
    "    # print(label_words_logits.shape)\n",
    "    # label_words_logits -= 10000*(1-label_words_mask)\n",
    "    mask = torch.zeros(len(tokenizer)) - 10000\n",
    "    print(label_words_logits)\n",
    "    for idx, logit in zip(label_words_ids, label_words_logits[0]):\n",
    "        mask[idx] = logit\n",
    "    return mask.unsqueeze(0)\n",
    "    # label1 = label_words_ids[0]\n",
    "    # label2 = label_words_ids[1]\n",
    "    # tmp = []\n",
    "    # tmp.append(logits.index_select(label1,axis=-1))\n",
    "    # tmp.append(logits.index_select(label2,axis=-1))\n",
    "    # label_words_logits = paddle.concat(tmp,axis=1).reshape([logits.shape[0],label_words_ids.shape[0],label_words_ids.shape[1]])\n",
    "    # label_words_logits -= 10000*(1-label_words_mask)\n",
    "    # return label_words_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(logits):\n",
    "    batch_size = logits.shape[0]\n",
    "    return F.softmax(logits.reshape([batch_size, -1]), dim=-1).reshape(logits.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calibrate(label_words_probs, calibrate_logits, **kwargs):\n",
    "    shape = label_words_probs.shape\n",
    "    assert calibrate_logits.dim() ==  1, \"self._calibrate_logits are not 1-d tensor\"\n",
    "    calibrate_label_words_probs = normalize(project([element for sublist in verb_tok for element in sublist], mask, calibrate_logits.unsqueeze(0), **kwargs))\n",
    "    print(calibrate_label_words_probs)\n",
    "    assert calibrate_label_words_probs.shape[1:] == label_words_probs.shape[1:] \\\n",
    "            and calibrate_label_words_probs.shape[0]==1, \"shape not match\"\n",
    "    print(label_words_probs.device, calibrate_label_words_probs.device)\n",
    "    print(label_words_probs)\n",
    "    print((calibrate_label_words_probs+1e-8))\n",
    "    label_words_probs /= (calibrate_label_words_probs+1e-3)\n",
    "    # normalize # TODO Test the performance\n",
    "    print(label_words_probs)\n",
    "    norm = label_words_probs.reshape(shape[0], -1).sum(dim=-1,keepdim=True) # TODO Test the performance of detaching()\n",
    "    print(norm)\n",
    "    label_words_probs = label_words_probs.reshape(shape[0], -1) / norm\n",
    "    label_words_probs = label_words_probs.reshape(*shape)\n",
    "    return label_words_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.utils.data.dataloader.DataLoader"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    0,     1,     2,  ..., 50254, 50255, 50256]), [[24561], [31591]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(mask == 1)[0], verb_tok"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "cc_logits = calibrate_with_ds(model, dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "output = model.generate(tokenizer.encode(text, return_tensors=\"pt\").to('cuda'), max_new_tokens=1, output_scores=True, return_dict_in_generate=True)\n",
    "logits = output.scores[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 1, 50257])"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits.unsqueeze(0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0.0113, -0.0019]], dtype=torch.float16)\n",
      "tensor([[0., 0., 0.,  ..., 0., 0., 0.]])\n",
      "cpu cpu\n",
      "tensor([[-1.2993e-02, -1.0979e-02,  3.1921e-02,  ..., -3.7537e-02,\n",
      "          3.0339e-05,  2.5635e-02]], dtype=torch.float16)\n",
      "tensor([[1.0000e-08, 1.0000e-08, 1.0000e-08,  ..., 1.0000e-08, 1.0000e-08,\n",
      "         1.0000e-08]])\n",
      "tensor([[-1.2992e+01, -1.0977e+01,  3.1922e+01,  ..., -3.7531e+01,\n",
      "          3.0334e-02,  2.5641e+01]], dtype=torch.float16)\n",
      "tensor([[-2596.]], dtype=torch.float16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[ 5.0049e-03,  4.2267e-03, -1.2299e-02,  ...,  1.4458e-02,\n",
       "         -1.1683e-05, -9.8801e-03]], dtype=torch.float16)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = calibrate(logits.cpu(), cc_logits.cpu())\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.2985e-02, -1.0963e-02,  3.1891e-02,  ..., -3.7506e-02,\n",
       "         3.0279e-05,  2.5620e-02], device='cuda:0', dtype=torch.float16)"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cc_logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([    2,     5,     6,  ..., 50252, 50255, 50256], device='cuda:0'),)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.where(cc_logits > 0)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-8.6427e-06,  1.4901e-06], dtype=torch.float16)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[0][[element for sublist in verb_tok for element in sublist]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
